{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        ">- K-Nearest Neighbors (KNN) is a simple, non-parametric, instance-based machine learning algorithm that makes predictions based on the K closest data points in the training set.\n",
        "How it works:\n",
        "\n",
        "\n",
        "1. Choose a value of K (number of neighbors).\n",
        "\n",
        "2. Calculate the distance (e.g., Euclidean) between the new data point and all training points.\n",
        "\n",
        "\n",
        "3. Select the K nearest neighbors.\n",
        "\n",
        "\n",
        ">- KNN for Classification:\n",
        "\n",
        "\n",
        "> The class is predicted by majority voting among the K neighbors.\n",
        "\n",
        "\n",
        "> Example: If most neighbors are “Default,” the output is “Default.”\n",
        "\n",
        "\n",
        ">- KNN for Regression:\n",
        "\n",
        "\n",
        "> The output is the average (or weighted average) of the K neighbors’ target values.\n",
        "\n",
        "\n",
        ">- Example: Predicting house price as the mean price of nearby houses.\n",
        "\n",
        "\n",
        ">- Key points:\n",
        "\n",
        "\n",
        "> No training phase (lazy learner)\n",
        "\n",
        "\n",
        "> Sensitive to K value and feature scaling\n",
        "\n",
        "\n",
        "> Works well with small, clean datasets\n",
        "\n",
        "Q2. What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        ">- Curse of Dimensionality refers to the problem that occurs when the number of features (dimensions) increases, making data points farther apart and sparse.\n",
        "\n",
        ">- Effect on KNN performance:\n",
        "\n",
        "> Distance measures become less meaningful in high dimensions.\n",
        "\n",
        "> Nearest neighbors are no longer truly “near.”\n",
        "\n",
        "> KNN accuracy decreases and computation becomes slower.\n",
        "\n",
        "> Result: KNN performs poorly on high-dimensional data unless dimensionality reduction (e.g., PCA) or feature selection is applied.\n",
        "\n",
        "Q3.  What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        ">- Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique that transforms original features into a smaller set of new uncorrelated variables (principal components) that capture maximum variance.\n",
        "\n",
        ">- Difference from Feature Selection:\n",
        "\n",
        "> PCA: Creates new features by combining existing ones (feature transformation).\n",
        "\n",
        "> Feature Selection: Chooses a subset of original features without changing them.\n",
        "\n",
        "> Key idea: PCA reduces dimensions by compression, while feature selection reduces dimensions by selection.\n",
        "\n",
        "Q4.  What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        ">- In PCA, eigenvectors and eigenvalues come from the covariance matrix of the data.\n",
        "\n",
        "- Eigenvectors: Represent the directions (principal components) along which the data varies the most.\n",
        "\n",
        "- Eigenvalues: Indicate the amount of variance captured along each eigenvector.\n",
        "\n",
        ">- Why they are important:\n",
        "\n",
        "> Eigenvectors define the new feature axes.\n",
        "\n",
        "> Eigenvalues help decide how many principal components to keep (higher eigenvalue = more information).\n",
        "\n",
        "Q5.  How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "Dataset:\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        ">- How KNN and PCA complement each other in one pipeline (Wine Dataset):\n",
        "\n",
        "PCA first: Reduces the Wine dataset’s high dimensionality by transforming features into fewer uncorrelated principal components, removing noise and redundancy.\n",
        "\n",
        "Then KNN: Runs on the reduced feature space where distance calculations are more meaningful.\n",
        "\n",
        ">Benefits together:\n",
        "\n",
        "Improves KNN accuracy by mitigating the curse of dimensionality\n",
        "\n",
        "Faster computation (fewer features)\n",
        "\n",
        "Better generalization and smoother decision boundaries\n",
        "\n",
        "In short:\n",
        "PCA simplifies the data → KNN performs better distance-based classification.\n",
        "\n",
        "Q6.  Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n"
      ],
      "metadata": {
        "id": "Z1IMvjjbnrTx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iSJcc2JnjfY",
        "outputId": "31080b36-9054-4111-84d2-5a2624047d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407407407407407\n",
            "Accuracy with scaling: 0.9629629629629629\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# KNN WITHOUT scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "acc_without_scaling = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# KNN WITH scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_with_scaling = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Results\n",
        "print(\"Accuracy without scaling:\", acc_without_scaling)\n",
        "print(\"Accuracy with scaling:\", acc_with_scaling)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n"
      ],
      "metadata": {
        "id": "WRBRDrmEBrNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Feature scaling\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "print(\"Explained Variance Ratio:\")\n",
        "print(pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWpYm-ybBl3K",
        "outputId": "7606c85f-27ce-40ea-dd72-dd5456e4d85e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "LOh1WLnwCD6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# KNN on original data\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "acc_original = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
        "\n",
        "# PCA (top 2 components)\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# KNN on PCA data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
        "\n",
        "# Results\n",
        "print(\"Accuracy on original dataset:\", acc_original)\n",
        "print(\"Accuracy on PCA dataset (2 components):\", acc_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82El6uejCCkj",
        "outputId": "f4bb7d36-c376-4a5c-8d77-1e1b20589993"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original dataset: 0.9629629629629629\n",
            "Accuracy on PCA dataset (2 components): 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.  Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results."
      ],
      "metadata": {
        "id": "2JGDV30RCWaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# KNN with Euclidean distance\n",
        "knn_eu = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_eu.fit(X_train_scaled, y_train)\n",
        "acc_eu = accuracy_score(y_test, knn_eu.predict(X_test_scaled))\n",
        "\n",
        "# KNN with Manhattan distance\n",
        "knn_man = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_man.fit(X_train_scaled, y_train)\n",
        "acc_man = accuracy_score(y_test, knn_man.predict(X_test_scaled))\n",
        "\n",
        "print(\"Accuracy (Euclidean):\", acc_eu)\n",
        "print(\"Accuracy (Manhattan):\", acc_man)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq3GdhzpCVBQ",
        "outputId": "a95be5d1-c2ce-4589-d192-8971f2dba604"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Euclidean): 0.9629629629629629\n",
            "Accuracy (Manhattan): 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        ">- 1. Using PCA to reduce dimensionality\n",
        "\n",
        "Gene expression data has thousands of correlated features.\n",
        "\n",
        "PCA transforms them into fewer uncorrelated components that retain maximum variance.\n",
        "\n",
        "This reduces noise, redundancy, and overfitting.\n",
        "\n",
        "2. Deciding how many components to keep\n",
        "\n",
        "Keep components that explain 90–95% variance.\n",
        "\n",
        "Use:\n",
        "\n",
        "Explained variance ratio\n",
        "\n",
        "Scree plot (optional)\n",
        "\n",
        "This balances information retention vs complexity.\n",
        "\n",
        "3. Using KNN after PCA\n",
        "\n",
        "Apply KNN on PCA-transformed data.\n",
        "\n",
        "Fewer dimensions → meaningful distance calculations.\n",
        "\n",
        "Improves accuracy and speed for distance-based models.\n",
        "\n",
        "4. Model evaluation\n",
        "\n",
        "Use train–test split or cross-validation.\n",
        "\n",
        "Metrics:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision / Recall (important in medical diagnosis)\n",
        "\n",
        "Confusion matrix\n",
        "\n",
        "5. Justification to stakeholders (business/clinical)\n",
        "\n",
        "✔ Reduces overfitting in small-sample, high-feature data\n",
        "\n",
        "✔ Improves model stability and generalization\n",
        "\n",
        "✔ Faster, interpretable, and clinically reliable\n",
        "\n",
        "✔ Suitable for real-world biomedical decision support\n"
      ],
      "metadata": {
        "id": "WPDlK9wNCoIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (proxy for gene expression data)\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# PCA (retain 95% variance)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "\n",
        "# Evaluation\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Number of PCA components:\", pca.n_components_)\n",
        "print(\"Total explained variance:\", pca.explained_variance_ratio_.sum())\n",
        "print(\"KNN Accuracy after PCA:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ5GFNbcCmxf",
        "outputId": "03fcdbc2-be75-43e1-b51e-5f01652d8d7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of PCA components: 10\n",
            "Total explained variance: 0.9513920521735783\n",
            "KNN Accuracy after PCA: 0.9649122807017544\n"
          ]
        }
      ]
    }
  ]
}